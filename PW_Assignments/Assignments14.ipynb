{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) What is Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Linear Regression is a statistical method that models the relationship between two variables by fitting a linear equation to observed data. It examines how the dependent variable changes as the independent variable changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What are the key assumptions of Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The key assumptions of Simple Linear Regression include:\n",
    "\n",
    "   - Linearity: The relationship between the independent and dependent variable is linear.\n",
    "\n",
    "   - Independence: The residuals (errors) are independent.\n",
    "\n",
    "   - Homoscedasticity: The residuals have constant variance at all levels of the independent variable.\n",
    "   \n",
    "   - Normality: The residuals of the model are normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What does the coefficient m represent in the equation Y=mX+c?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the equation Y = mX + c, the coefficient m represents the slope of the regression line. It quantifies the change in the dependent variable Y for every one-unit change in the independent variable X.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. What does the intercept c represent in the equation Y=mX+c?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept c represents the expected value of Y when the independent variable X is zero. It is the point where the regression line crosses the Y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. How do we calculate the slope m in Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The slope m is calculated using the formula: \n",
    "\n",
    "   \\[\n",
    "   m = \\frac{N(\\sum XY) - (\\sum X)(\\sum Y)}{N(\\sum X^2) - (\\sum X)^2}\n",
    "   \\]\n",
    "   \n",
    "   where N is the number of observations, and X and Y are the independent and dependent variable values, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. What is the purpose of the least squares method in Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The least squares method aims to minimize the sum of the squared differences (residuals) between the observed values and the values predicted by the linear regression model. This ensures the best-fitting line for the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of determination (R²) measures the proportion of variance in the dependent variable that can be explained by the independent variable. An R² value close to 1 indicates a strong relationship, while a value close to 0 suggests a weak relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. What is Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Multiple Linear Regression is an extension of Simple Linear Regression that models the relationship between two or more independent variables and a dependent variable by fitting a linear equation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. What is the main difference between Simple and Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference is that Simple Linear Regression uses one independent variable to predict the dependent variable, while Multiple Linear Regression uses two or more independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. What are the key assumptions of Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The key assumptions of Multiple Linear Regression include:\n",
    "\n",
    "    - Linearity: The relationship between the independent variables and the dependent variable is linear.\n",
    "\n",
    "    - Independence: The residuals are independent of each other.\n",
    "\n",
    "    - Homoscedasticity: There is constant variance of the residuals.\n",
    "\n",
    "    - No multicollinearity: The independent variables are not highly correlated with one another.\n",
    "    \n",
    "    - Normality: The residuals are normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heteroscedasticity refers to the scenario where the variance of the residuals is not constant across all levels of the independent variable(s). It can lead to inefficient estimates and affect the statistical significance of the model's coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. How can you improve a Multiple Linear Regression model with high multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  To improve a model with high multicollinearity, you can:\n",
    "\n",
    "    - Remove one of the correlated variables.\n",
    "\n",
    "    - Combine correlated variables through techniques like principal component analysis (PCA).\n",
    "    \n",
    "    - Use regularization methods such as Ridge or Lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. What are some common techniques for transforming categorical variables for use in regression models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Common techniques include:\n",
    "\n",
    "    - One-hot encoding: Creating binary columns for each category.\n",
    "\n",
    "    - Label encoding: Assigning each category a unique integer.\n",
    "    \n",
    "    - Using ordinal encoding: For ordinal categories, assigning ranks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. What is the role of interaction terms in Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction terms allow the model to capture the effect of one independent variable on the dependent variable, depending on the level of another independent variable. This helps to account for more complex relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In Simple Linear Regression, the intercept represents the expected outcome when the independent variable is zero. In Multiple Linear Regression, it can be less interpretable as it only holds meaning in the context of the other independent variables being equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. What is the significance of the slope in regression analysis, and how does it affect predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The slope indicates the strength and direction of the relationship between an independent variable and the dependent variable. A steep slope indicates a stronger relationship, leading to larger changes in predictions with changes in the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. How does the intercept in a regression model provide context for the relationship between variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The intercept contextualizes the model by providing a baseline value of the dependent variable when all independent variables are set to zero, though it may not always be meaningful in practical situations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. What are the limitations of using R² as a sole measure of model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  R² does not account for model complexity and can be artificially inflated with more predictors. It does not indicate whether the model is adequate or if the assumptions of regression analysis hold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. How would you interpret a large standard error for a regression coefficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large standard error indicates that there is considerable variability in the estimate of the coefficient, suggesting that the coefficient may not be a reliable predictor of the dependent variable or that there is multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Heteroscedasticity can be identified by plotting residuals against predicted values or independent variables; if there is a pattern (like a funnel shape), it indicates heteroscedasticity. Addressing it is important because it can lead to inefficient estimates and invalid statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A high R² with low adjusted R² suggests that the model may be overfitting by including too many predictors, which are not contributing significantly to the explanation of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. Why is it important to scale variables in Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Scaling variables ensures that different ranges of data do not disproportionately influence the model, especially in the presence of regularization techniques and when variables have different units or scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23. What is polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Polynomial regression is a type of regression analysis where the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24. How does polynomial regression differ from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Unlike linear regression, which fits a straight line, polynomial regression can fit curves, allowing for a more flexible modeling of relationships that are not linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. When is polynomial regression used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Polynomial regression is used when data shows a nonlinear relationship that can be better modeled with a polynomial function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26. What is the general equation for polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The general equation for polynomial regression of degree n is:\n",
    "\n",
    "    \\[\n",
    "\n",
    "    Y = c + m_1X + m_2X^2 + m_3X^3 + ... + m_nX^n\n",
    "    \n",
    "    \\]\n",
    "\n",
    "    where \\( Y \\) is the dependent variable, \\( c \\) is the intercept, and \\( m_1, m_2, ..., m_n \\) are the coefficients for each term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27. Can polynomial regression be applied to multiple variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Yes, polynomial regression can be applied to multiple variables by including interaction and polynomial terms for each independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28. What are the limitations of polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitations of polynomial regression include overfitting, difficulty in interpretation, and the potential to produce unrealistic predictions (especially at the extremes of the data range)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Methods include using cross-validation to assess predictive performance, looking at adjusted R², and considering the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30. Why is visualization important in polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization allows for the assessment of how well the polynomial curve fits the data, helping to identify potential overfitting and the appropriateness of the chosen degree of the polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31. How is polynomial regression implemented in Python?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression can be implemented in Python using libraries like `numpy` for creating polynomial features through `PolynomialFeatures` and `sklearn` for fitting the model with `LinearRegression`. Here's a basic implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 1, Predicted: 1.0000000000000009\n",
      "Actual: 4, Predicted: 3.9999999999999956\n",
      "Actual: 9, Predicted: 8.999999999999996\n",
      "Actual: 16, Predicted: 16.0\n",
      "Actual: 25, Predicted: 25.00000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1, 4, 9, 16, 25])\n",
    "\n",
    "# Transforming data for polynomial regression\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Fitting the polynomial regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# Making predictions\n",
    "predictions = model.predict(X_poly)\n",
    "\n",
    "# Displaying the actual vs predicted values\n",
    "for actual, predicted in zip(y, predictions):\n",
    "    print(f\"Actual: {actual}, Predicted: {predicted}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
