{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is a parameter?\n",
    "\n",
    "A parameter is a numerical characteristic of a population or a statistical model that is used to describe or define the properties of the population or the model. Parameters are often estimated from sample data and are used to make inferences about the population.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What is correlation?\n",
    "\n",
    "Correlation is a statistical measure that describes the relationship between two continuous variables. It measures the degree to which the variables tend to move together. Correlation coefficients range from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What does negative correlation mean?\n",
    "\n",
    "A negative correlation between two variables means that as one variable increases, the other variable tends to decrease. For example, there may be a negative correlation between the amount of rain in a region and the number of people who go to the beach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Machine Learning. What are the main components in Machine Learning?\n",
    "\n",
    "Machine learning is a subfield of artificial intelligence that involves training algorithms to learn from data and make predictions or decisions without being explicitly programmed.\n",
    "The main components of machine learning are:\n",
    "\n",
    "Data: The input data used to train the model.\n",
    "\n",
    "Model: The algorithm or mathematical function that is trained on the data.\n",
    "\n",
    "Loss function: A function that measures the difference between the model's predictions and the actual values.\n",
    "\n",
    "Optimization algorithm: An algorithm that adjusts the model's parameters to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. How does loss value help in determining whether the model is good or not?\n",
    "\n",
    "The loss value, also known as the error or cost function, measures the difference between the model's predictions and the actual values. A lower loss value indicates that the model is better at making predictions. By monitoring the loss value during training, you can determine whether the model is improving or not. \n",
    "\n",
    "If the loss value is high, it may indicate that the model is not well-suited to the problem or that the data is noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. What are continuous and categorical variables?\n",
    "\n",
    "Continuous variables: Variables that can take on any value within a given range or interval, such as height, weight, or temperature.\n",
    "\n",
    "Categorical variables: Variables that can take on only a limited number of distinct values or categories, such as gender, color, or type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "\n",
    "Categorical variables need to be converted into numerical variables before they can be used in machine learning algorithms. Common techniques for handling categorical variables include:\n",
    "\n",
    "One-hot encoding: Creating a new binary variable for each category.\n",
    "\n",
    "Label encoding: Assigning a numerical value to each category.\n",
    "\n",
    "Ordinal encoding: Assigning a numerical value to each category based on its order or rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. What do you mean by training and testing a dataset?\n",
    "\n",
    "Training: The process of using a dataset to train a machine learning model, where the model learns to make predictions or decisions based on the input data.\n",
    "\n",
    "Testing: The process of evaluating the performance of a trained machine learning model on a separate dataset, where the model's predictions are compared to the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. What is sklearn.preprocessing?\n",
    "\n",
    "sklearn.preprocessing is a module in the scikit-learn library that provides functions for preprocessing data, such as scaling, normalization, and encoding categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. What is a Test set?\n",
    "\n",
    "A test set, also known as a holdout set, is a portion of a dataset that is set aside and not used during training. \n",
    "The test set is used to evaluate the performance of a trained machine learning model, providing an unbiased estimate of its performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. How do we split data for model fitting (training and testing) in Python?\n",
    "\n",
    "In Python, you can split data for model fitting using the train_test_split function from the sklearn.model_selection module. \n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model on the training data\n",
    "model = LogisticRegression(max_iter=1000) # add max_iter parameter\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 How do you approach a Machine Learning problem?\n",
    "Approaching a machine learning problem involves several steps:\n",
    "\n",
    "Problem Definition : Clearly define the problem you want to solve.\n",
    "\n",
    "Data Collection : Gather relevant data for the problem.\n",
    "\n",
    "Data Preprocessing : Clean, transform, and prepare the data for modeling.\n",
    "\n",
    "Exploratory Data Analysis (EDA) : Understand the distribution, relationships, and patterns in the data.\n",
    "\n",
    "Feature Engineering : Select and create relevant features from the data.\n",
    "\n",
    "Model Selection : Choose a suitable machine learning algorithm.\n",
    "\n",
    "Model Training : Train the model using the prepared data.\n",
    "\n",
    "Model Evaluation : Evaluate the performance of the trained model.\n",
    "\n",
    "Model Tuning : Fine-tune hyperparameters to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  13) Why do we have to perform EDA before fitting a model to the data?\n",
    "\n",
    "Performing Exploratory Data Analysis (EDA) before fitting a model is crucial because it:\n",
    "\n",
    "Helps understand the distribution of variables.\n",
    "\n",
    "Identifies missing values, outliers, and anomalies.\n",
    "\n",
    "Reveals relationships and correlations between variables.\n",
    "\n",
    "Informs feature engineering and selection.\n",
    "\n",
    "Guides model selection and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14)  What is correlation?\n",
    "Correlation measures the strength and direction of the linear relationship between two continuous variables. \n",
    "\n",
    "It ranges from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15) What does negative correlation mean?\n",
    "Negative correlation indicates that as one variable increases, the other variable tends to decrease. \n",
    "\n",
    "For example, there might be a negative correlation between the amount of rainfall and the number of ice cream sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16) How can you find correlation between variables in Python?\n",
    "You can use the corr() function from pandas or the pearsonr() function from scipy.stats to calculate\n",
    "\n",
    " the correlation coefficient between two variables in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17)  What is causation? Explain difference between correlation and causation with an example.\n",
    "Causation implies that one variable (cause) directly affects another variable (effect). Correlation does not necessarily imply causation.\n",
    "\n",
    "Example:\n",
    "\n",
    "Correlation: There might be a positive correlation between the number of ice cream sales and the number of sunglasses sold. \n",
    "\n",
    "However, eating ice cream does not cause people to buy sunglasses.\n",
    "\n",
    "Causation: There is a causal relationship between smoking (cause) and lung cancer (effect)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18) What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "\n",
    "An optimizer is an algorithm that adjusts model parameters to minimize the loss function.\n",
    "\n",
    "Types of optimizers:\n",
    "\n",
    "Gradient Descent (GD): Updates parameters based on the gradient of the loss function.\n",
    "\n",
    "Example: optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "Stochastic Gradient Descent (SGD): A variant of GD that uses a single sample to compute the gradient.\n",
    "\n",
    "Example: optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "Momentum: Adds a fraction of the previous update to the current update.\n",
    "\n",
    "Example: optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "Nesterov Accelerated Gradient (NAG): Modifies the momentum update rule.\n",
    "\n",
    "Example: optimizer = optim.Nesterov(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "Adagrad: Adapts the learning rate for each parameter based on the gradient.\n",
    "\n",
    "Example: optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "\n",
    "RMSprop: Divides the learning rate by a moving average of the squared gradient.\n",
    "\n",
    "Example: optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "\n",
    "Adam: Combines the benefits of Adagrad and RMSprop.\n",
    "\n",
    "Example: optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19) What is sklearn.linear_model ?\n",
    "sklearn.linear_model is a module in scikit-learn that provides implementations of various linear models, including:\n",
    "\n",
    "Linear Regression\n",
    "\n",
    "Ridge Regression\n",
    "\n",
    "Lasso Regression\n",
    "\n",
    "Elastic Net Regression\n",
    "\n",
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20) What does model.fit() do? What arguments must be given?\n",
    "\n",
    "model.fit() trains the model on the provided data. The required arguments are:\n",
    "\n",
    "X: The feature data.\n",
    "\n",
    "y: The target data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21) What does model.predict() do? What arguments must be given?\n",
    "model.predict() uses the trained model to make predictions on new, unseen data. The required argument is:\n",
    "\n",
    "X: The new feature data to make predictions on.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22) What are continuous and categorical variables?\n",
    "In statistics and machine learning, variables can be classified into two main types:\n",
    "\n",
    "Continuous Variables : These variables can take any value within a range or interval, including fractions and decimals. \n",
    "\n",
    "Examples include height, weight, temperature, and blood pressure.\n",
    "\n",
    "Categorical Variables : These variables can only take specific, distinct values. \n",
    "\n",
    "They are often represented as strings or integers. Examples include gender, nationality, color, and occupation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23. What is feature scaling? How does it help in Machine Learning?\n",
    "Feature scaling is a technique used in Machine Learning to normalize the range of independent variables or features of data. It is also known as normalization.\n",
    "\n",
    "Feature scaling helps in Machine Learning by:\n",
    "\n",
    "Improving the performance of models that rely on distance calculations, such as K-Nearest Neighbors (KNN) and Support Vector Machines (SVM).\n",
    "\n",
    "Preventing features with large ranges from dominating the model.\n",
    "\n",
    "Speeding up the convergence of gradient descent algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24. How do we perform scaling in Python?\n",
    "To perform scaling in Python, you can use the StandardScaler class from the sklearn.preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2\n",
      "0 -1.414214 -1.414214\n",
      "1 -0.707107 -0.707107\n",
      "2  0.000000  0.000000\n",
      "3  0.707107  0.707107\n",
      "4  1.414214  1.414214\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {'Feature1': [1, 2, 3, 4, 5],\n",
    "        'Feature2': [11, 12, 13, 14, 15]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "print(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. What is sklearn.preprocessing?\n",
    "sklearn.preprocessing is a module in scikit-learn that provides various functions and classes for data preprocessing, including feature scaling, normalization, encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26. How do we split data for model fitting (training and testing) in Python?\n",
    "To split data for model fitting (training and testing) in Python, you can use the train_test_split function from the sklearn.model_selection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "   Feature1  Feature2\n",
      "4         5        15\n",
      "2         3        13\n",
      "0         1        11\n",
      "3         4        14\n",
      "4    1\n",
      "2    0\n",
      "0    0\n",
      "3    1\n",
      "Name: Target, dtype: int64\n",
      "\n",
      "Testing Data:\n",
      "   Feature1  Feature2\n",
      "1         2        12\n",
      "1    0\n",
      "Name: Target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {'Feature1': [1, 2, 3, 4, 5],\n",
    "        'Feature2': [11, 12, 13, 14, 15],\n",
    "        'Target': [0, 0, 0, 1, 1]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df[['Feature1', 'Feature2']]\n",
    "y = df['Target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "print(\"\\nTesting Data:\")\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27. Explain data encoding?\n",
    "Data encoding is the process of converting categorical data into numerical data that can be processed by machine learning algorithms.\n",
    "\n",
    "Types of Encoding\n",
    "There are several types of encoding techniques, including:\n",
    "\n",
    "Label Encoding: assigns a unique integer value to each category.\n",
    "\n",
    "One-Hot Encoding: creates a binary vector for each category.\n",
    "\n",
    "Ordinal Encoding: assigns a unique integer value to each category, \n",
    "\n",
    "preserving the ordinal relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color\n",
      "0      2\n",
      "1      1\n",
      "2      0\n",
      "3      2\n",
      "4      1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {'Color': ['Red', 'Green', 'Blue', 'Red', 'Green']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "df['Color'] = le.fit_transform(df['Color'])\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
